import os
import streamlit as st
import google.genai as genai
from google.genai.errors import APIError

# RAG iÃ§in gerekli kÃ¼tÃ¼phaneler
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# --- SABÄ°T AYARLAR ---
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2" # HÄ±zlÄ± ve yerel bir embedding modeli
VECTOR_DB_FILE = "faiss_index.bin"
PDF_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "veri_seti", "ucak_kontrol_sistemleri.pdf")

# API AnahtarÄ±nÄ± Streamlit Secrets'tan gÃ¼venli bir ÅŸekilde al
try:
    API_KEY = st.secrets["GEMINI_API_KEY"]
    # 20. satÄ±r: Genellikle bu eski bir LangChain veya SDK metoduydu.
    # En gÃ¼ncel SDK'da bu satÄ±ra gerek yoktur, Client objesi otomatik yapÄ±landÄ±rÄ±lÄ±r.
    # Bu satÄ±rÄ± SÄ°LÄ°N veya YORUM SATIRI yapÄ±n.
    # genai.configure(api_key=API_KEY) 
except KeyError:
    st.error("HATA: GEMINI_API_KEY Streamlit Secrets'ta tanÄ±mlÄ± deÄŸil! LÃ¼tfen secrets.toml dosyasÄ±nÄ± kontrol edin.")
    API_KEY = None

# --- RAG SÄ°STEMÄ° KURULUMU (Cache EdilmiÅŸ) ---

@st.cache_resource
def setup_rag_system():
    if not API_KEY:
        return None, None

    # 1. Metni PDF'ten Ã‡Ä±karma
    try:
        reader = PdfReader(PDF_PATH)
        text = "".join(page.extract_text() for page in reader.pages)
    except Exception as e:
        st.error(f"HATA: PDF yÃ¼klenemedi veya okunamadÄ±: {e}")
        return None, None

    # 2. Metni ParÃ§alama (Basit Metin ParÃ§alayÄ±cÄ±)
    sentences = [s.strip() for s in text.split('.') if s.strip()]

    # 3. Gemini Embedding Modelini HazÄ±rlama
    embedding_model_name = "text-embedding-004"
    client = genai.Client(api_key=API_KEY)
    
    # 4. GÃ¶mmeleri OluÅŸturma ve FAISS Ä°ndeksi Kurma
    st.info("Ä°lk kullanÄ±mda gÃ¼Ã§lÃ¼ Gemini embedding modeli ile indeks oluÅŸturuluyor. LÃ¼tfen bekleyin...")
    
    # BATCH BOYUTU: Gemini'nin limiti 100 olduÄŸu iÃ§in 90 kullanÄ±yoruz.
    BATCH_SIZE = 90
    all_embeddings = []

    try:
        # Metinleri 90'ar kiÅŸilik gruplara ayÄ±rma (Batching)
        for i in range(0, len(sentences), BATCH_SIZE):
            batch = sentences[i:i + BATCH_SIZE]
            
            # Gemini API Ã§aÄŸrÄ±sÄ±
            response = client.models.embed_content(
                model=embedding_model_name,
                contents=batch # DoÄŸru parametre adÄ±
            )
            
            # YENÄ° VE GÃœVENLÄ° KOD BAÅLANGICI (Boyut KontrolÃ¼)
            
            # Gelen gÃ¶mmeleri NumPy'a Ã§eviriyoruz
            current_embeddings = np.array(response.embeddings)
            
            # GÃœVENLÄ°K KONTROLÃœ: Boyut doÄŸru deÄŸilse (768), bu batch'i atla.
            # Gemini'nin embedding boyutu 768'dir.
            if current_embeddings.shape[1] == 768:
                all_embeddings.append(current_embeddings)
            else:
                st.warning(f"UYARI: {i+1}. batch'te boyut tutarsÄ±zlÄ±ÄŸÄ± bulundu. AtlanÄ±yor. (Boyut: {current_embeddings.shape[1]})")
                
            st.caption(f"Ä°lerleme: {i + len(batch)}/{len(sentences)} cÃ¼mle gÃ¶mmesi tamamlandÄ±.")
            
            # YENÄ° VE GÃœVENLÄ° KOD SONU
        
    except Exception as e:
        # EÄŸer API Ã§aÄŸrÄ±sÄ±nda hata olursa (AÄŸ/API AnahtarÄ±)
        st.error(f"Gemini Embedding HatasÄ±: {e}")
        return None, None, None # Hata durumunda fonksiyonu sonlandÄ±r
    
    # ******* Ã–NEMLÄ°: GÄ°RÄ°NTÄ° BURADA BÄ°TÄ°YOR *******
    
    # 5. GÃ¼venlik KontrolÃ¼ ve NumPy'a DÃ¶nÃ¼ÅŸtÃ¼rme (DoÄŸru Girinti Seviyesi)
    if not all_embeddings:
        st.error("HATA: Gemini API'den hiÃ§bir gÃ¶mme (embedding) alÄ±namadÄ±. LÃ¼tfen API anahtarÄ±nÄ±zÄ± veya PDF iÃ§eriÄŸini kontrol edin.")
        return None, None, None

    try:
        # NumPy'a gÃ¼venli dÃ¶nÃ¼ÅŸtÃ¼rme iÃ§in np.vstack kullanma ve FAISS iÃ§in float32'ye zorlama
        embeddings = np.vstack(all_embeddings).astype(np.float32) 
    except ValueError as e:
        st.error(f"Embeddings dizisi oluÅŸturulurken hata: {e}. Muhtemelen boÅŸ bir gÃ¶mme geldi.")
        return None, None, None

    # FAISS indeksi oluÅŸturma (Bu kÄ±sÄ±m da fonksiyonun ana girinti seviyesinde olmalÄ±)
    dimension = embeddings.shape[1] 
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    
    return embedding_model_name, index, sentences

# --- CEVAP OLUÅTURMA FONKSÄ°YONU ---

def generate_rag_response(prompt, embedding_model_name, index, sentences):
    client = genai.Client(api_key=API_KEY)
    
    # 1. Prompt'un GÃ¶mmesini OluÅŸturma (Gemini ile)
    try:
        query_response = client.models.embed_content(
            model=embedding_model_name,
            contents=[prompt]
        )
        query_embedding = np.array(query_response.embeddings)
    except Exception as e:
        return f"Sorgu GÃ¶mmesi HatasÄ±: {e}"
    
    # 2. En YakÄ±n ParÃ§alarÄ± FAISS ile Arama
    k = 4 # BaÄŸlamÄ± artÄ±rmak iÃ§in 4 parÃ§ayÄ± alalÄ±m
    distances, indices = index.search(query_embedding, k)
    
    # 3. BaÄŸlamÄ± (Context) BirleÅŸtirme
    context_chunks = [sentences[i] for i in indices[0]]
    context = "\n".join(context_chunks)

    # 4. Gemini iÃ§in Prompt Åablonu
    system_prompt = (
        "Sen bir uÃ§ak sistemleri uzmanÄ±sÄ±n. YalnÄ±zca aÅŸaÄŸÄ±daki BAÄLAMI kullanarak sorularÄ± TÃ¼rkÃ§e yanÄ±tla. "
        "CevabÄ± baÄŸlamda bulamazsan 'Verilen baÄŸlamda bu bilgi bulunmamaktadÄ±r.' de."
        "\n\n--- BAÄLAM ---\n"
        f"{context}"
    )
    
    # 5. Gemini API Ã‡aÄŸrÄ±sÄ±
    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=[
                {"role": "user", "parts": [{"text": system_prompt}]},
                {"role": "user", "parts": [{"text": "Soru: " + prompt}]}
            ],
            config={"temperature": 0.1}
        )
        return response.text

    except APIError as e:
        return f"Gemini API HatasÄ±: {e}"
    except Exception as e:
        return f"Beklenmedik Hata: {e}"


# --- STREAMLIT ARAYÃœZÃœ ---

def main():
    st.set_page_config(page_title="Acil RAG Chatbot", layout="wide")
    st.title("Acil UÃ§ak Kontrol Sistemleri RAG Chatbot ğŸš€")
    st.caption("Veri KaynaÄŸÄ±: UÃ§ak Kontrol Sistemleri PDF'i (LangChain'siz Acil Ã‡Ã¶zÃ¼m)")

    # 1. RAG Sistemini Kurma
    rag_data = setup_rag_system()
    if rag_data is None:
        return
    embedding_model, index, sentences = rag_data

    # 2. Sohbet GeÃ§miÅŸini BaÅŸlatma
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "Merhaba! Proje teslimi iÃ§in acil durum modu devrede. UÃ§uÅŸ kontrol sistemleri hakkÄ±nda ne sormak istersiniz?"}
        ]

    # 3. Sohbet GeÃ§miÅŸini GÃ¶rÃ¼ntÃ¼leme
    for msg in st.session_state["messages"]:
        st.chat_message(msg["role"]).write(msg["content"])

    # 4. KullanÄ±cÄ± GiriÅŸini Ä°ÅŸleme
    if prompt := st.chat_input("Sorunuzu buraya yazÄ±n..."):
        st.session_state["messages"].append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)

        with st.spinner("Cevap aranÄ±yor..."):
            yanit = generate_rag_response(prompt, embedding_model, index, sentences)
            
            st.session_state["messages"].append({"role": "assistant", "content": yanit})
            st.chat_message("assistant").write(yanit)

if __name__ == "__main__":
    main()