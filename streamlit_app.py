import os
import streamlit as st
from functools import lru_cache

# LangChain ekirdek ve Topluluk Bileenleri
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

# Yeni LCEL (Expression Language) Zincir Fonksiyonlar覺
# 襤ki fonksiyonu da dorudan topluluk paketinin k繹k羹nden 癟ekmeyi deniyoruz.
from langchain_community.chains import create_stuff_documents_chain
from langchain_community.chains import create_retrieval_chain

# --- SAB襤T AYARLAR ---
GEMINI_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "embedding-001"
VECTOR_DB_DIR = "./chroma_db"

# GVENL襤 PDF YOLU TANIMLAMA
current_dir = os.path.dirname(os.path.abspath(__file__))
PDF_PATH = os.path.join(current_dir, "veri_seti", "ucak_kontrol_sistemleri.pdf")

# API Anahtar覺n覺 Streamlit Secrets'tan g羹venli bir ekilde al
try:
    API_KEY = st.secrets["GEMINI_API_KEY"]
    # Pydantic v1 hatas覺n覺 atlamak i癟in ortam deikenini zorla tan覺mla
    os.environ["GEMINI_API_KEY"] = API_KEY 
except KeyError:
    st.error("HATA: GEMINI_API_KEY Streamlit Secrets'ta tan覺ml覺 deil! L羹tfen secrets.toml dosyas覺n覺 kontrol edin.")
    API_KEY = None

# G繹mme fonksiyonunu (Embedding Function) sadece API anahtar覺 varsa olutur
if API_KEY:
    EMBEDDING_FUNCTION = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)
else:
    EMBEDDING_FUNCTION = None


@st.cache_resource
def setup_rag_system():
    if EMBEDDING_FUNCTION is None:
        return None

    # 1. LLM ve Embedding Modelini Tan覺mlama
    llm = ChatGoogleGenerativeAI(
        model=GEMINI_MODEL,  
        temperature=0.1,
        # API anahtar覺 zaten os.environ'da olduu i癟in burada tekrar vermeye gerek yok
    )

    # 2. Veri Y羹kleme
    try:
        loader = PyPDFLoader(PDF_PATH)
        documents = loader.load()
    except Exception as e:
        st.error(f"HATA: PDF y羹klenirken bir sorun olutu. Dosya yolu: {e}")
        return None

    # 3. Metin Par癟alama (Chunking)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = text_splitter.split_documents(documents)

    # 4. Vekt繹r Veritaban覺 Oluturma ve Retriever Tan覺mlama
    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=EMBEDDING_FUNCTION,
        persist_directory=VECTOR_DB_DIR
    )
    retriever = vector_store.as_retriever()

    # 5. Yeni LCEL RAG Zincirini Kurma (RetrievalQA yerine)
    
    # Prompt ablonu
    system_prompt = (
        "Sen bir u癟ak sistemleri uzman覺s覺n. Yaln覺zca verilen balam覺 kullanarak sorular覺 T羹rk癟e yan覺tla. "
        "Cevab覺 balamda bulamazsan 'Verilen balamda bu bilgi bulunmamaktad覺r.' de."
        "\n\n{context}"
    )

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}"),
        ]
    )

    # Balam覺 Birletirme Zinciri
    combine_docs_chain = create_stuff_documents_chain(llm, prompt)

    # Nihai Retrieval Zinciri
    qa_chain = create_retrieval_chain(
        retriever, 
        combine_docs_chain
    )
    return qa_chain


# --- STREAMLIT ARAYZ ---

def main():
    # 1. Sayfa Ayarlar覺
    st.set_page_config(page_title="RAG Chatbot", layout="wide")
    st.title("U癟ak Kontrol Sistemleri RAG Chatbot ")
    st.caption("Veri Kayna覺: U癟ak Kontrol Sistemleri PDF'i")

    # 2. RAG Sistemini Kurma
    qa_chain = setup_rag_system()
    
    if qa_chain is None:
        return

    # 3. Sohbet Ge癟miini Balatma
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "Merhaba! U癟u kontrol sistemleri hakk覺nda ne sormak istersiniz?"}
        ]

    # 4. Sohbet Ge癟miini G繹r羹nt羹leme
    for msg in st.session_state["messages"]:
        st.chat_message(msg["role"]).write(msg["content"])

    # 5. Kullan覺c覺 Giriini 襤leme
    if prompt := st.chat_input("Sorunuzu buraya yaz覺n..."):
        # Kullan覺c覺 mesaj覺n覺 ekle
        st.session_state["messages"].append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)

        # Cevab覺 羹retme
        with st.spinner("Cevap aran覺yor..."):
            try:
                # Yeni LCEL zincirini 癟a覺rma y繹ntemi
                response = qa_chain.invoke({"input": prompt})
                yanit = response['answer']
                
                # Asistan cevab覺n覺 ekle
                st.session_state["messages"].append({"role": "assistant", "content": yanit})
                st.chat_message("assistant").write(yanit)
            
            except Exception as e:
                # Hata durumunda mesaj
                hata_mesaji = f"zg羹n羹m, RAG zincirinde bir hata olutu: {e}"
                st.session_state["messages"].append({"role": "assistant", "content": hata_mesaji})
                st.chat_message("assistant").write(hata_mesaji)

# --- UYGULAMAYI BALATMA ---
if __name__ == "__main__":
    main()